{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
     
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 20.2.3; however, version 20.2.4 is available.\n",
      "You should consider upgrading via the 'c:\\users\\nisha\\anaconda3\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt \n",
    "\n",
    "#import torch\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertModel, BertForMaskedLM\n",
    "from spacy.tokenizer import Tokenizer\n",
    "from spacy.lang.en import English\n",
    "\n",
    "def replace_tokens(text, tokenised_text, keywords, punctuation_list, mode):\n",
    "    keyword_replacements = []\n",
    "    for i, t in enumerate(keywords):\n",
    "        target_word = str(t)\n",
    "        # ignore if it's just punctuation\n",
    "        if target_word in punctuation_list:\n",
    "            continue\n",
    "        # ignore if it's not the list of BERT tokens\n",
    "        if target_word.lower() not in tokenised_text:\n",
    "            continue\n",
    "            \n",
    "        target_token = target_word.lower()\n",
    "        masked_index = tokenised_text.index(target_token)\n",
    "        tokenised_text[masked_index] = '[MASK]'\n",
    "\n",
    "        indexed_tokens = tokeniser.convert_tokens_to_ids(tokenised_text)\n",
    "        segments_ids = [1] * len(tokenised_text)\n",
    "        segments_ids[0] = 0\n",
    "        segments_ids[1] = 0\n",
    "\n",
    "        # Convert inputs to PyTorch tensors\n",
    "        tokens_tensor = torch.tensor([indexed_tokens])\n",
    "        segments_tensors = torch.tensor([segments_ids])\n",
    "\n",
    "        # Predict all tokens\n",
    "        predictions = model(tokens_tensor, segments_tensors)\n",
    "\n",
    "        chosen_token = \"X\"\n",
    "        while True:\n",
    "            predicted_index = torch.argmax(predictions[0, masked_index]).item()\n",
    "            predicted_token = tokeniser.convert_ids_to_tokens([predicted_index])[0]\n",
    "            if predicted_token not in punctuation_list and predicted_token.find(target_token) < 0 and target_token.find(predicted_token) < 0:\n",
    "                try:\n",
    "                    ri = keyword_replacements.index(predicted_token)\n",
    "                except ValueError:\n",
    "                    ri = -1\n",
    "                if predicted_token == 'bell':\n",
    "                    print(\"pt: \" + predicted_token)\n",
    "                    print(f'ri: {ri}')\n",
    "                    print(f'kr: {keyword_replacements[ri]}')\n",
    "                    print(f'k: {keywords[ri]}')\n",
    "                    print(f'k: {keywords[i]}')\n",
    "                if ri < 0 or (ri >= 0 and keywords[i] == keywords[ri]):\n",
    "                    # and keyword_replacements[ri].find(predicted_token) < 0 and predicted_token.find(keyword_replacements[ri]) < 0)\n",
    "                    chosen_word = predicted_token\n",
    "                    #print(f'{target_word} -> {chosen_word}')\n",
    "                    break\n",
    "                else:\n",
    "                    predictions[0,masked_index,predicted_index] = -9999\n",
    "            else:\n",
    "                predictions[0,masked_index,predicted_index] = -9999\n",
    "\n",
    "        if mode == 'all':\n",
    "            current_text_tokens = spacy_tokeniser(text)\n",
    "            text = ' '.join([str(spacy_tokeniser(chosen_word)) if str(token) == target_word else str(token) for token in current_text_tokens])\n",
    "        else:\n",
    "            text = text.replace(target_word, chosen_word, 1)\n",
    "            \n",
    "        tokenised_text = tokeniser.tokenize(text)\n",
    "        keyword_replacements.append(chosen_word)\n",
    "        \n",
    "    return text\n",
    "    \n",
    "\n",
    "model_path = \"bert-base-uncased\"\n",
    "model = BertForMaskedLM.from_pretrained(model_path)\n",
    "model.eval()\n",
    "\n",
    "tokeniser = BertTokenizer.from_pretrained(model_path)\n",
    "#f1 = open(\"data/hello_world.txt\", \"r\")\n",
    "#text = \"dummy. \" + f1.read()\n",
    "#text = ''\n",
    "#text = \"How are you planning to go the college today?\"\n",
    "#text = \"dummy. \" + \"I am in the college.\"\n",
    "#text = \"dummy. \" + \"How are you?\"\n",
    "#tokenised_text = tokeniser.tokenize(text)\n",
    "a_file = open(\"data/InputFile.txt\",\"r\")\n",
    "lines = a_file.readlines()\n",
    "\n",
    "f2 = open(\"data/keywords.txt\", \"r\")\n",
    "keywords = f2.read().split()\n",
    "\n",
    "\n",
    "'''nlp = English()\n",
    "spacy_tokeniser = nlp.Defaults.create_tokenizer(nlp)\n",
    "keywords = spacy_tokeniser(text.replace('dummy. ','',1))'''\n",
    "\n",
    "punctuation = [\n",
    "    ',',\n",
    "    '.',\n",
    "    '!',\n",
    "    '?',\n",
    "    ':',\n",
    "    ';',\n",
    "    '...',\n",
    "    '\\n', \n",
    "    '\\n\\n',\n",
    "    '\"\"'\n",
    "]\n",
    "\n",
    "file1 = open(r\"synthetic_data_MURALI.txt\",\"a\")\n",
    "\n",
    "for k in range(1):   #change the range to get more data points\n",
    "    generated_lines = []\n",
    "    final_generated_lines = []\n",
    "    for i in range((k*10),(k*10) + 10):\n",
    "        tokenised_text = tokeniser.tokenize(lines[i])\n",
    "        text = replace_tokens(lines[i], tokenised_text, keywords, punctuation, \"selection\")\n",
    "        new_line = (str(text.split('\\n')))\n",
    "    \n",
    "        generated_lines.append(new_line)\n",
    "    for j in range(len(generated_lines)):\n",
    "        final_generated_lines.append(str(''.join(generated_lines[j]).strip('[]')))\n",
    "        file1.writelines(str(final_generated_lines[j]))\n",
    "        file1.write(\"\\n\")\n",
    "\n",
    "file1.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
